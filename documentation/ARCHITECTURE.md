# ARCHITECTURE

## List of main technologies:
* NodeJs
* GraphQL
* REST
* PostgreSQL
* REDIS
* RabbitMq

## Entities list
* Users (Myself as a user state: user + auth token)
* Posts (Publications, reposts, etc.)
* Comments
* Organizations (communities)
* Tags
* Blockchain nodes - for caching blockchain state


## Entities states (response fields structure):
* `Full` - all possible fields
* `Preview` - amount of fields required for lists of entities
* `Card` - minimum amount of fields, used in `object inside object` situations, 
ex. `entity_for` - for whom post is published, User or Organization data.

It is possible that in future such states will completely be moved to frontend. Because of GraphQL implementation.

## Statistics module

### Goals:
* Event-based model. Save as more as possible raw data.
* Never delete any data in order to be able to make retrospective research and compare old values of old algorithm
and new values of new calculation algorithm.
* Easy and quick access to entity current stats values to show on website pages.


### Solutions

#### Database architecture

There is a table entity_event_param which is used to store `statistics events`. Examples of such events:
* saving of entity current parameter (current importance, current upvotes amount, etc.)
* calculation of entity parameter (importance delta for current value and value before 24 hours, etc.)

Notes:
* There are no any updates. Possible operations are inserts/selects.
* There is a relational database, same as main database (PostgreSQL). It is not required yet to implement such solutions
as ClickHouse, because required period of updating is `1 hour`.
* Statistics events are completely separated from `users activity` - another kinds of events, generated by user.
Statistic event - is event only about some values.

Scaling strategy:
* This table is placed to separate database in order to move this database to separate DB server in future.
* Partitioning for `hot values` (required for calculation).
* Sharding for `cold values` (required for retrospective.
* Migration to ClickHouse using existing Db schema.


#### Workers

Now there are cron-like workers only. In future it is possible to use queue-like workers (event-based, not schedule-based ones)

There are different kinds of workers:
* External data worker (ex. importance worker) - this worker fetches data from external sources (ex. blockchain)
and saves data to database, at most to `entity_event_params`. (But not always due to existing legacy solutions)
* Internal data worker - this worker fetches data from internal sources (main relational database) and saves it
to `entity_event_param`. Example - fetch current upvotes and save it.
* Worker-calculator - it uses only `entity_event_param` data to calculate new values. Example - calculate importance
delta.
* Worker-saver - it is used to save current entity stats values to concrete entity tables. For example, update (here update is appeared)
current importance delta for the concrete post. In order to use this value to select posts lists ordered by importance delta
(frontend application is a client for such request). Sometimes it is better to save such values by `worker-calculator`,
because it has all required values.


#### Code implementation
TODO - provide links to code with explanation of ideas
